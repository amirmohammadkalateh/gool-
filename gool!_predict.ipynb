{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqdUEGDV39HXIrA72sfeXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirmohammadkalateh/gool-/blob/main/gool!_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Set a seed for reproducibility\n",
        "np.random.seed(42)\n",
        "keras.utils.set_random_seed(42)\n",
        "\n",
        "# Step 2: Load and Prepare Data.\n",
        "print(\"Loading data from 'final_dataset.csv'...\")\n",
        "try:\n",
        "    data = pd.read_csv('final_dataset.csv')\n",
        "    print(\"Data loaded successfully. Here's a quick look at the first 5 rows:\")\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'final_dataset.csv' not found. Please ensure the file is in the same directory.\")\n",
        "    exit()\n",
        "\n",
        "# Drop the 'Date' column as it's a string and not directly useful for a numeric model.\n",
        "if 'Date' in data.columns:\n",
        "    data = data.drop('Date', axis=1)\n",
        "\n",
        "# Define the features (X) and the target variable (y).\n",
        "X = data.drop('AQI', axis=1)\n",
        "y = data['AQI']\n",
        "\n",
        "# Split the data into training and testing sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the feature data.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "def create_model(input_shape, optimizer_name='adam', init_mode='he_uniform', dropout_rate=0.2,\n",
        "                 l1_reg=0.01, l2_reg=0.01, clipvalue=1.0):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer and first hidden layer with Batch Normalization\n",
        "    model.add(Dense(64, input_shape=input_shape, kernel_initializer=init_mode,\n",
        "                    activation='relu', kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),\n",
        "                    kernel_constraint=MaxNorm(3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Second hidden layer with Batch Normalization\n",
        "    model.add(Dense(32, activation='relu', kernel_initializer=init_mode,\n",
        "                    kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg),\n",
        "                    kernel_constraint=MaxNorm(3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer (for regression)\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # optimizer\n",
        "    optimizer = Adam(clipvalue=clipvalue)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Perform a Manual Grid Search.\n",
        "# i define the hyperparameters to search over.\n",
        "param_grid = {\n",
        "    'dropout_rate': [0.1, 0.2],\n",
        "    'l1_reg': [0.001, 0.01],\n",
        "    'l2_reg': [0.001, 0.01],\n",
        "    'clipvalue': [1.0, 0.5],\n",
        "    'batch_size': [16, 32],\n",
        "    'epochs': [50, 100]\n",
        "}\n",
        "\n",
        "best_score = float('inf')\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "input_shape = (X_train_scaled.shape[1],)\n",
        "\n",
        "print(\"\\nStarting manual Grid Search for hyperparameter tuning. This may take some time...\")\n",
        "\n",
        "# Iterate through each hyperparameter combination\n",
        "for dropout_rate in param_grid['dropout_rate']:\n",
        "    for l1_reg in param_grid['l1_reg']:\n",
        "        for l2_reg in param_grid['l2_reg']:\n",
        "            for clipvalue in param_grid['clipvalue']:\n",
        "                for batch_size in param_grid['batch_size']:\n",
        "                    for epochs in param_grid['epochs']:\n",
        "\n",
        "                        # Create the model with the current hyperparameters\n",
        "                        model = create_model(input_shape=input_shape,\n",
        "                                             dropout_rate=dropout_rate,\n",
        "                                             l1_reg=l1_reg,\n",
        "                                             l2_reg=l2_reg,\n",
        "                                             clipvalue=clipvalue)\n",
        "\n",
        "                        # Define the EarlyStopping callback\n",
        "                        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "                        # Train the model with the current hyperparameters\n",
        "                        history = model.fit(X_train_scaled, y_train,\n",
        "                                            batch_size=batch_size,\n",
        "                                            epochs=epochs,\n",
        "                                            validation_split=0.1,\n",
        "                                            callbacks=[early_stopping],\n",
        "                                            verbose=0)\n",
        "\n",
        "                        # Evaluate the model on the validation set\n",
        "                        val_loss = min(history.history['val_loss'])\n",
        "\n",
        "                        print(f\"Tested params: Dropout={dropout_rate}, L1={l1_reg}, L2={l2_reg}, Clip={clipvalue}, Batch={batch_size}, Epochs={epochs}, Val Loss={val_loss:.4f}\")\n",
        "\n",
        "                        # Check if this is the best model so far\n",
        "                        if val_loss < best_score:\n",
        "                            best_score = val_loss\n",
        "                            best_params = {\n",
        "                                'dropout_rate': dropout_rate,\n",
        "                                'l1_reg': l1_reg,\n",
        "                                'l2_reg': l2_reg,\n",
        "                                'clipvalue': clipvalue,\n",
        "                                'batch_size': batch_size,\n",
        "                                'epochs': epochs\n",
        "                            }\n",
        "                            best_model = model\n",
        "\n",
        "print(\"\\n--- Manual Grid Search Results ---\")\n",
        "print(f\"Best validation loss: {best_score:.4f}\")\n",
        "print(\"Best parameters found: \", best_params)\n",
        "\n",
        "# Evaluate the best model\n",
        "if best_model:\n",
        "    test_loss, test_mae = best_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "    print(f\"Final best model evaluation on test data:\")\n",
        "    print(f\"  Test Loss (MSE): {test_loss:.4f}\")\n",
        "    print(f\"  Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = best_model.predict(X_test_scaled)\n",
        "    print(\"\\nExample predictions vs actual values:\")\n",
        "    for i in range(5):\n",
        "        print(f\"  Predicted: {predictions[i][0]:.2f}, Actual: {y_test.iloc[i]:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASWzNDAaXQ4P",
        "outputId": "76e13bc2-8a7a-41dc-c042-875fe46af2f2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from 'final_dataset.csv'...\n",
            "Data loaded successfully. Here's a quick look at the first 5 rows:\n",
            "   Date  Month  Year  Holidays_Count  Days   PM2.5    PM10     NO2    SO2  \\\n",
            "0     1      1  2021               0     5  408.80  442.42  160.61  12.95   \n",
            "1     2      1  2021               0     6  404.04  561.95   52.85   5.18   \n",
            "2     3      1  2021               1     7  225.07  239.04  170.95  10.93   \n",
            "3     4      1  2021               0     1   89.55  132.08  153.98  10.42   \n",
            "4     5      1  2021               0     2   54.06   55.54  122.66   9.70   \n",
            "\n",
            "     CO  Ozone  AQI  \n",
            "0  2.77  43.19  462  \n",
            "1  2.60  16.43  482  \n",
            "2  1.40  44.29  263  \n",
            "3  1.01  49.19  207  \n",
            "4  0.64  48.88  149  \n",
            "\n",
            "Starting manual Grid Search for hyperparameter tuning. This may take some time...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tested params: Dropout=0.1, L1=0.001, L2=0.001, Clip=1.0, Batch=16, Epochs=50, Val Loss=3395.6484\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.001, Clip=1.0, Batch=16, Epochs=100, Val Loss=3028.9111\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.001, Clip=1.0, Batch=32, Epochs=50, Val Loss=12521.9336\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.001, Clip=1.0, Batch=32, Epochs=100, Val Loss=2667.8389\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.001, Clip=0.5, Batch=16, Epochs=50, Val Loss=2622.7410\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.001, Clip=0.5, Batch=16, Epochs=100, Val Loss=2503.2568\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.001, Clip=0.5, Batch=32, Epochs=50, Val Loss=11730.9141\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.001, Clip=0.5, Batch=32, Epochs=100, Val Loss=2149.5513\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.01, Clip=1.0, Batch=16, Epochs=50, Val Loss=3316.8320\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.01, Clip=1.0, Batch=16, Epochs=100, Val Loss=2249.6790\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.01, Clip=1.0, Batch=32, Epochs=50, Val Loss=13429.0566\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.01, Clip=1.0, Batch=32, Epochs=100, Val Loss=3382.9827\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.01, Clip=0.5, Batch=16, Epochs=50, Val Loss=3069.1919\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.01, Clip=0.5, Batch=16, Epochs=100, Val Loss=2653.9651\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.01, Clip=0.5, Batch=32, Epochs=50, Val Loss=12603.3672\n",
            "Tested params: Dropout=0.1, L1=0.001, L2=0.01, Clip=0.5, Batch=32, Epochs=100, Val Loss=2693.5737\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.001, Clip=1.0, Batch=16, Epochs=50, Val Loss=2680.3086\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.001, Clip=1.0, Batch=16, Epochs=100, Val Loss=3087.3037\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.001, Clip=1.0, Batch=32, Epochs=50, Val Loss=11869.7080\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.001, Clip=1.0, Batch=32, Epochs=100, Val Loss=2253.7395\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.001, Clip=0.5, Batch=16, Epochs=50, Val Loss=2536.5981\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.001, Clip=0.5, Batch=16, Epochs=100, Val Loss=2030.5439\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.001, Clip=0.5, Batch=32, Epochs=50, Val Loss=12322.9238\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.001, Clip=0.5, Batch=32, Epochs=100, Val Loss=2241.0679\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.01, Clip=1.0, Batch=16, Epochs=50, Val Loss=2381.8938\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.01, Clip=1.0, Batch=16, Epochs=100, Val Loss=2468.7471\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.01, Clip=1.0, Batch=32, Epochs=50, Val Loss=12316.1045\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.01, Clip=1.0, Batch=32, Epochs=100, Val Loss=2125.7947\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.01, Clip=0.5, Batch=16, Epochs=50, Val Loss=3346.5251\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.01, Clip=0.5, Batch=16, Epochs=100, Val Loss=4319.4771\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.01, Clip=0.5, Batch=32, Epochs=50, Val Loss=11811.4863\n",
            "Tested params: Dropout=0.1, L1=0.01, L2=0.01, Clip=0.5, Batch=32, Epochs=100, Val Loss=2396.9675\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.001, Clip=1.0, Batch=16, Epochs=50, Val Loss=3592.1865\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.001, Clip=1.0, Batch=16, Epochs=100, Val Loss=3105.0291\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.001, Clip=1.0, Batch=32, Epochs=50, Val Loss=12841.0918\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.001, Clip=1.0, Batch=32, Epochs=100, Val Loss=3225.4009\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.001, Clip=0.5, Batch=16, Epochs=50, Val Loss=3909.8232\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.001, Clip=0.5, Batch=16, Epochs=100, Val Loss=3976.5378\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.001, Clip=0.5, Batch=32, Epochs=50, Val Loss=13369.3848\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.001, Clip=0.5, Batch=32, Epochs=100, Val Loss=3304.2529\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.01, Clip=1.0, Batch=16, Epochs=50, Val Loss=3001.0051\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.01, Clip=1.0, Batch=16, Epochs=100, Val Loss=3189.2461\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.01, Clip=1.0, Batch=32, Epochs=50, Val Loss=13005.0615\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.01, Clip=1.0, Batch=32, Epochs=100, Val Loss=2956.4990\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.01, Clip=0.5, Batch=16, Epochs=50, Val Loss=3651.8958\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.01, Clip=0.5, Batch=16, Epochs=100, Val Loss=2609.8684\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.01, Clip=0.5, Batch=32, Epochs=50, Val Loss=13173.5957\n",
            "Tested params: Dropout=0.2, L1=0.001, L2=0.01, Clip=0.5, Batch=32, Epochs=100, Val Loss=3906.7617\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.001, Clip=1.0, Batch=16, Epochs=50, Val Loss=3250.5244\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.001, Clip=1.0, Batch=16, Epochs=100, Val Loss=2732.9055\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.001, Clip=1.0, Batch=32, Epochs=50, Val Loss=13506.5195\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.001, Clip=1.0, Batch=32, Epochs=100, Val Loss=3593.9409\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.001, Clip=0.5, Batch=16, Epochs=50, Val Loss=3551.4675\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.001, Clip=0.5, Batch=16, Epochs=100, Val Loss=3140.5947\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.001, Clip=0.5, Batch=32, Epochs=50, Val Loss=13646.7988\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.001, Clip=0.5, Batch=32, Epochs=100, Val Loss=3844.9514\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.01, Clip=1.0, Batch=16, Epochs=50, Val Loss=3095.6465\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.01, Clip=1.0, Batch=16, Epochs=100, Val Loss=2562.5676\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.01, Clip=1.0, Batch=32, Epochs=50, Val Loss=13029.6846\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.01, Clip=1.0, Batch=32, Epochs=100, Val Loss=3572.2688\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.01, Clip=0.5, Batch=16, Epochs=50, Val Loss=3041.5247\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.01, Clip=0.5, Batch=16, Epochs=100, Val Loss=2990.1067\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.01, Clip=0.5, Batch=32, Epochs=50, Val Loss=12736.4512\n",
            "Tested params: Dropout=0.2, L1=0.01, L2=0.01, Clip=0.5, Batch=32, Epochs=100, Val Loss=2958.3577\n",
            "\n",
            "--- Manual Grid Search Results ---\n",
            "Best validation loss: 2030.5439\n",
            "Best parameters found:  {'dropout_rate': 0.1, 'l1_reg': 0.01, 'l2_reg': 0.001, 'clipvalue': 0.5, 'batch_size': 16, 'epochs': 100}\n",
            "Final best model evaluation on test data:\n",
            "  Test Loss (MSE): 996.2709\n",
            "  Test MAE: 22.2933\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "\n",
            "Example predictions vs actual values:\n",
            "  Predicted: 197.98, Actual: 176.00\n",
            "  Predicted: 284.00, Actual: 314.00\n",
            "  Predicted: 241.17, Actual: 258.00\n",
            "  Predicted: 294.57, Actual: 262.00\n",
            "  Predicted: 372.54, Actual: 459.00\n"
          ]
        }
      ]
    }
  ]
}